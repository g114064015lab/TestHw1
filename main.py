import numpy as np
import pandas as pd
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# ----------------------------
# Config & Intro (README-aligned)
# ----------------------------
st.set_page_config(page_title="Linear Regression (CRISP-DM)", page_icon="ðŸ“ˆ", layout="centered")
st.title("ðŸ“ˆ Simple Linear Regression â€” CRISP-DM Demo")

with st.expander("1) Business Understanding"):
    st.markdown(
        """
æœ¬æ‡‰ç”¨ç”¨æ–¼æŽ¢ç´¢ **å–®ä¸€æ•¸å€¼ç‰¹å¾µ X èˆ‡ç›®æ¨™ y** çš„ç·šæ€§é—œä¿‚ï¼Œä¸¦ä»¥**ç°¡å–®ç·šæ€§å›žæ­¸**å»ºç«‹é æ¸¬æ¨¡åž‹ã€‚  
å…¸åž‹æƒ…å¢ƒå¦‚ï¼šé ç®—â†’éŠ·å”®ã€åªæ•¸â†’æˆ¿åƒ¹ã€‚æ­¤äº’å‹•å¼ä»‹é¢å¯å¿«é€Ÿé«”é©—ä¸¦ç†è§£ç·šæ€§å›žæ­¸æ¦‚å¿µã€‚
"""
    )

with st.expander("2) Data Understanding (Synthetic)"):
    st.markdown(
        """
è³‡æ–™ç”±æ–¹ç¨‹å¼ **y = aÂ·x + b**ï¼ˆæ­¤è™•å›ºå®š b=0 ä»¥ç°¡åŒ–ï¼‰åŠ ä¸Šé«˜æ–¯é›œè¨Šæ‰€ç”Ÿæˆï¼Œ  
ä½ å¯ä»¥å³æ™‚èª¿æ•´ **æ–œçŽ‡ a**ã€**é›œè¨Šå¤§å°**ã€**è³‡æ–™é»žæ•¸** ä¾†è§€å¯Ÿå­¸ç¿’é›£æ˜“åº¦çš„è®ŠåŒ–ã€‚
"""
    )


# ----------------------------
# Sidebar controls
# ----------------------------
st.sidebar.header("Parameters")
a = st.sidebar.slider("Slope (a)", min_value=-10.0, max_value=10.0, value=2.0, step=0.1)
noise = st.sidebar.slider("Noise (std dev)", min_value=0.0, max_value=10.0, value=1.0, step=0.1)
n_points = st.sidebar.slider("Number of points", min_value=20, max_value=2000, value=200, step=10)
test_size = st.sidebar.slider("Test size", min_value=0.1, max_value=0.5, value=0.2, step=0.05)  # README é è¨­ 0.2
seed = st.sidebar.number_input("Random state", value=42, step=1)

# ----------------------------
# Data generation
# ----------------------------
def generate_data(a: float, n_points: int, noise: float, seed: int = 42):
    """
    ç”¢ç”Ÿåˆæˆè³‡æ–™ï¼š
      X ~ linspace(0, 10, n_points)
      y = a * X + Normal(0, noise^2)
    å›žå‚³å½¢ç‹€ï¼š
      X: (n, 1), y: (n,)
    """
    rng = np.random.default_rng(seed)
    X = np.linspace(0, 10, n_points)
    y = a * X + rng.normal(0, noise, n_points)
    return X.reshape(-1, 1), y

# ----------------------------
# Train / evaluate
# ----------------------------
def train_and_evaluate_model(
    X: np.ndarray,
    y: np.ndarray,
    test_size: float = 0.2,
    seed: int = 42
):
    """
    åˆ‡åˆ†è³‡æ–™ã€è¨“ç·´ç·šæ€§å›žæ­¸ï¼Œè¼¸å‡ºæ¨¡åž‹èˆ‡æ¸¬è©¦é›† MSE åŠå„åˆ†å‰²è³‡æ–™ã€‚
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=seed
    )
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred_test = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred_test)
    return model, mse, X_train, y_train, X_test, y_test, y_pred_test

# ----------------------------
# Outlier detection (no extra UI)
# ----------------------------
def detect_outliers_by_zscore(residuals: np.ndarray, z_thresh: float = 2.5) -> np.ndarray:
    """
    ä»¥æ®˜å·®çš„ Z åˆ†æ•¸åµæ¸¬é›¢ç¾¤é»žã€‚å›žå‚³å¸ƒæž—é®ç½©ï¼ˆTrue è¡¨ç¤ºé›¢ç¾¤ï¼‰ã€‚
    - åƒ…ç”¨ã€Œæ¸¬è©¦é›†æ®˜å·®ã€è¨ˆç®—å¹³å‡èˆ‡æ¨™æº–å·®ã€‚
    """
    mu = float(np.mean(residuals))
    sigma = float(np.std(residuals, ddof=1)) if len(residuals) > 1 else 0.0
    if sigma == 0:
        return np.zeros_like(residuals, dtype=bool)
    z = (residuals - mu) / sigma
    return np.abs(z) > z_thresh

# ----------------------------
# Main flow
# ----------------------------
# ç”¢ç”Ÿè³‡æ–™ä¸¦è¨“ç·´
X, y = generate_data(a, n_points, noise, seed=int(seed))
model, mse, X_train, y_train, X_test, y_test, y_pred_test = train_and_evaluate_model(
    X, y, test_size=float(test_size), seed=int(seed)
)

# å–å¾—æ¸¬è©¦æ®˜å·®èˆ‡é›¢ç¾¤é»žé®ç½©ï¼ˆè‡ªå‹•ï¼‰
residuals_test = y_test - y_pred_test
outlier_mask = detect_outliers_by_zscore(residuals_test, z_thresh=2.5)

# ----------------------------
# Results & coefficients
# ----------------------------
st.subheader("Results (Evaluation)")
st.write(f"**Mean Squared Error (Test):** {mse:.4f}")

st.subheader("Model Coefficients")
st.write(
    f"- **Intercept:** {model.intercept_:.4f}\n"
    f"- **Slope (coef for X):** {model.coef_[0]:.4f}"
)

# ----------------------------
# Plot: training/testing + regression line + highlighted outliers
# ----------------------------
fig, ax = plt.subplots(figsize=(8.4, 6.2))

# Training / Testing
ax.scatter(X_train, y_train, label='Training data', alpha=0.75)
ax.scatter(X_test, y_test, label='Testing data', alpha=0.75)

# Regression line (fit on training)
x_line = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
y_line = model.predict(x_line)
ax.plot(x_line, y_line, linewidth=2.2, label='Regression line')

# Highlight & annotate outliers on TEST set (no extra UI)
if np.any(outlier_mask):
    ax.scatter(
        X_test[outlier_mask],
        y_test[outlier_mask],
        s=90,
        facecolor="#6A0DAD",
        edgecolor="black",
        alpha=0.95,
        label="Detected Outliers"
    )
    # ç°¡çŸ­æ¨™è¨»ï¼šOutlier #k
    for i, (xx, yy) in enumerate(zip(X_test[outlier_mask, 0], y_test[outlier_mask])):
        ax.annotate(
            f"Outlier",
            (xx, yy),
            textcoords="offset points",
            xytext=(6, -10),
            fontsize=9,
            color="#6A0DAD",
            weight="bold",
        )

ax.set_title('Simple Linear Regression (Train/Test & Auto-Highlighted Outliers)')
ax.set_xlabel('X')
ax.set_ylabel('y')
ax.legend()
ax.grid(alpha=0.15)
st.pyplot(fig)

# ----------------------------
# Top 5 Outliers table (by |residual| on TEST)
# ----------------------------
st.subheader("Top 5 Outliers (Test Set)")
outlier_df = pd.DataFrame({
    "X": X_test.flatten(),
    "y_true": y_test,
    "y_pred": y_pred_test,
    "residual": residuals_test,
    "abs_residual": np.abs(residuals_test),
    "flagged_by_zscore": outlier_mask
}).sort_values("abs_residual", ascending=False).head(5).reset_index(drop=True)

st.dataframe(outlier_df)

# ----------------------------
# Helpful notes (concise)
# ----------------------------
st.caption(
    "Notes: Outliers are detected automatically from **test residuals** using z-score > 2.5. "
    "If many points are flagged, consider reducing noise, increasing sample size, or checking data quality."
)
